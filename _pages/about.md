---
layout: about
title: about
permalink: /
subtitle: 

profile:
  align: right
  image: face2.jpg
  image_circular: false # crops the image to make it circular
  more_info: >

news: true # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi! I am a *final year* Ph.D candidate at **Seoul National University** advised by Prof. [Kyomin Jung](http://milab.snu.ac.kr/publication.html) and a student researcher at **<span style="color:#4285F4;">G</span><span style="color:#EA4335;">o</span><span style="color:#FBBC05;">o</span><span style="color:#4285F4;">g</span><span style="color:#34A853;">l</span><span style="color:#EA4335;">e</span>**. I also have also interned at **<a href="https://lawzero.org/en" style="color:green;">LawZero</a>** and **<a href='https://europe.naverlabs.com/' style="color:blue;">NAVER Labs Europe </a>**.

My research centers on ***Safe and Controllable Agentic AI systems***:

1. **RL & its variant algorithms** ‚Äî *Distributional Matching RL* for **RLHF** ([CriticControl](https://aclanthology.org/2023.findings-acl.281), [AdvisorQA](https://aclanthology.org/2025.naacl-long.333/), [Drift](https://arxiv.org/pdf/2502.14289)) and **RLVR** ([GUARD](https://openreview.net/pdf?id=8roRgrjbjv), [PACED-RL](https://arxiv.org/abs/2602.12642)),

2. **Construction of safety guardrails** ([LifeTox](https://aclanthology.org/2024.naacl-short.60.pdf), [CausalArmor](https://arxiv.org/pdf/2602.07918)) and **benchmarks** ([AdvisorQA](https://aclanthology.org/2025.naacl-long.333/), [VLind-Bench](https://aclanthology.org/2025.findings-naacl.231/)),

3. And its **applications to AI agents** ([ReflAct](https://aclanthology.org/2025.emnlp-main.1697/), [Syntra](https://arxiv.org/abs/2509.17393), [CausalArmor](https://arxiv.org/pdf/2602.07918)) and practical deployments ([SumGD](https://aclanthology.org/2025.findings-naacl.235/), [CCI](https://aclanthology.org/2025.findings-acl.82/), [Drift](https://arxiv.org/pdf/2502.14289)).

In particular, I'm interested in ***how to prevent harm from misalignment in future, more capable AI Agents***.<br>
Please feel free to reach out to me for a discussion here **minbeomkim@snu.ac.kr**!

*üìç I am currently seeking Research Scientist positions (or equivalent roles) starting in mid-2026.*

<!-- My Ph.D. research primarily focuses on ***Controlled Generation*** from Large Language Models. More specifically, how can we control generations that are 1) [guaranteed to strictly satisfy essential constraints (e.g. safety guarantee)](), 2) aligned with [personal preferences](), and 3) still [preserving pre-aligned general preferences]()? As a practical and impactful solution for challenges, I strongly believe that harmony **between training-time methods** and **inference-time methods** is essential. Although it looks deeply challenging, I am enjoying this journey and looking forward to the new insights it will bring. üòé -->

