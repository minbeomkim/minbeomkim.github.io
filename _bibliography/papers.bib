---
---

@article{kim2024advisorqa,
      title={AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence}, 
      author={Kim, Minbeom and Lee, Hwanhee and Park, Joonsuk and Lee, Hwaran and Jung, Kyomin},
      month={Apr},
      year={2024},
      eprint={2404.11826},
      abstract={As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity.},
      journal={arXiv},
      pdf={https://arxiv.org/abs/2404.11826},
      preview={AdvisorQA.png}
}


@article{kim2023lifetox,
  title={LifeTox: Unveiling Implicit Toxicity in Life Advice},
  author={Kim, Minbeom and Koo, Jahyun and Lee, Hwanhee and Park†, Joonsuk and Lee, Hwaran and Jung†, Kyomin},
  journal={NAACL 2024},
  pdf={https://arxiv.org/pdf/2311.09585v2.pdf},
  abstract={As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity.},
  month={jun},
  year={2024},
  dataset={https://huggingface.co/datasets/mbkim/LifeTox},
  model={https://huggingface.co/mbkim/LifeTox_Moderator_13B},
  preview={LifeTox.png}
}

@article{kim-etal-2023-critic,
    title = "Critic-Guided Decoding for Controlled Text Generation",
    author = "Kim, Minbeom  and
      Lee, Hwanhee  and
      Yoo, Kang Min  and
      Park, Joonsuk  and
      Lee†, Hwaran  and
      Jung†, Kyomin",
    journal = "ACL 2023 (Findings Spotlights)",
    month = "jul",
    year = "2023",
    pdf = "https://aclanthology.org/2023.findings-acl.281",
    code = "https://github.com/minbeomkim/CriticControl",
    slides = "https://minbeomkim.github.io/assets/pdf/ACL2023-CriticControl-Slides.pdf",
    abstract = "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
    preview = "CriticControl.png"
}

@article{kim2022action,
  title={Action-driven contrastive representation for reinforcement learning},
  author={Kim, Minbeom and Rho, Kyeongha and Kim, Yong-duk and Jung†, Kyomin},
  journal={Plos one (IF=3.7)},
  volume={17},
  number={3},
  pages={e0265456},
  month={mar},
  year={2022},
  pdf={https://minbeomkim.github.io/assets/pdf/ADAT.pdf},
  abstract={In reinforcement learning, reward-driven feature learning directly from high-dimensional images faces two challenges: sample-efficiency for solving control tasks and generalization to unseen observations. In prior works, these issues have been addressed through learning representation from pixel inputs. However, their representation faced the limitations of being vulnerable to the high diversity inherent in environments or not taking the characteristics for solving control tasks. To attenuate these phenomena, we propose the novel contrastive representation method, Action-Driven Auxiliary Task (ADAT), which forces a representation to concentrate on essential features for deciding actions and ignore control-irrelevant details. In the augmented state-action dictionary of ADAT, the agent learns representation to maximize agreement between observations sharing the same actions. The proposed method significantly outperforms model-free and model-based algorithms in the Atari and OpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization.},
  publisher={Public Library of Science San Francisco, CA USA},
  preview={ADAT.png}
}

