---
---

@article{kim2024drift,
      title={üèéÔ∏è Drift: Decoding-time Personalized Alignments with Implicit User Preferences}, 
      author={Kim, Minbeom and Lee, Kang-il and Joo, Seongho and Lee, Hwaran and Jung, Kyomin},
      year={2025},
      month={Jul},
      journal={Under review},
      abstract={Personalized alignments towards individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding-time with implicit user preferences. Unlike traditional Reinforcement Learning from Human Feedback (RLHF), which relies on vast annotated datasets and expensive gradient updates, Drift operates in a training-free manner by steering a frozen LLM through few-shot preference modeling. Our approach represents user preferences as a composition of interpretable and predefined attributes, and employs a zero-shot rewarding mechanism based on contrastive system prompts. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift achieves performance comparable to standard RLHF methods while using only 50‚Äì100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable.},
      preview={Drift.png}
}

@article{kim2024guaranteed,
      title={Guaranteed Generation from Large Language Models}, 
      author={Kim, Minbeom and Thonet, Thibaut and Rozen, Jos and Lee, Hwaran and Jung, Kyomin and Dymetman, Marc},
      year={2025},
      month={Apr},
      journal={ICLR 2025},
      abstract={As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible? We first define the ideal distribution - the one closest to the original model, which also always satisfies the expressed constraint - as the ultimate goal of guaranteed generation. We then state a fundamental limitation, namely that it is impossible to reach that goal through autoregressive training alone. This motivates the necessity of combining training-time and inference-time methods to enforce such guarantees. Based on this insight, we propose GUARD, a simple yet effective approach that combines an autoregressive proposal distribution with rejection sampling. Through GUARD's theoretical properties, we show how controlling the KL divergence between a specific proposal and the target ideal distribution simultaneously optimizes inference speed and distributional closeness. To validate these theoretical concepts, we conduct extensive experiments on two text generation settings with hard-to-satisfy constraints: a lexical constraint scenario and a sentiment reversal scenario. These experiments show that GUARD achieves perfect constraint satisfaction while almost preserving the ideal distribution with highly improved inference efficiency. GUARD provides a principled approach to enforcing strict guarantees for LLMs without compromising their generative capabilities.},
      pdf={https://openreview.net/forum?id=8roRgrjbjv},
      preview={GUARD.png}
}


@article{kim2024advisorqa,
      title={AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence}, 
      author={Kim, Minbeom and Lee, Hwanhee and Park, Joonsuk and Lee‚Ä†, Hwaran and Jung‚Ä†, Kyomin},
      year={2025},
      abstract={As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity.},
      journal={NAACL 2025},
      code={https://github.com/minbeomkim/AdvisorQA},
      dataset={https://huggingface.co/datasets/mbkim/AdvisorQA},
      pdf={https://arxiv.org/abs/2404.11826},
      preview={AdvisorQA3.png}
}

@article{nips,
      title={Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding}, 
      author={Min, Kyungmin and Kim, Minbeom and Lee, Kang-il and Lee, Dongryeol and Jung, Kyomin},
      year={2025},
      journal={NAACL 2025 Findings},
      abstract={Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks. However, they struggle with object hallucinations due to over-reliance on learned textual patterns, ignoring the provided image. To address this issue, we first investigate language priors in LVLMs. We observe two key findings: (1) Even when predicting image-related part-of-speech (POS) tokens, models increasingly rely on linguistic priors as the token sequences grow, thereby amplifying hallucinations. (2) Methods that directly control LVLM's output distribution to mitigate language priors can lead to a degradation in text quality or exacerbate hallucinations. Based on these insights, we propose Summary-Guided Decoding (SGD). This method naturally encourages the model to focus more on the image information, with control over only the image-related POS tokens for preserving text quality. Through experiments, we demonstrate that SGD achieves state-of-the-art performance on object hallucination benchmarks. Furthermore, while existing methods show a trade-off between precision and recall, SGD proves to be Pareto optimal in this respect. Lastly, we show that while existing methods suffer from text quality degradation due to such trade-offs, SGD preserves text quality to the maximum extent possible. This paper not only focuses on preventing object hallucination but also presents analysis and solutions aimed at maintaining the original properties of LVLMs.},
      pdf={https://arxiv.org/abs/2410.13321},      
      preview={SGD.png}
}

@article{lee2024VlindBenchML,
      title={VLind-Bench: Measuring Language Priors in Large Vision-Language Models}, 
      author={Lee, Kang-il and Kim, Minbeom and Yoon, Seunghyun and Kim, Minsung and Lee, Dongryeol and Koh, Hyukhun and Jung, Kyomin},
      year={2025},
      abstract={Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks. However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information. Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution. Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied. Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors. To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs. It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as instance comprehension, visual perception, and commonsense biases. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language prior, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors.},
      journal={NAACL 2025 Findings},
      code={https://github.com/klee972/VLind-Bench},
      pdf={https://arxiv.org/pdf/2406.08702},
      dataset={https://huggingface.co/datasets/klee972/VLind-Bench},
      preview={VLind.png}
}


@article{park2024character,
      title={A Character-Centric Creative Story Generation via Imagination}, 
      author={Park, Kyeongman and Kim, Minbeom and Jung, Kyomin},
      year={2024},
      abstract={Creative story generation with diverse and detailed story elements is a long-standing goal for large language models. While existing methodologies generate long and coherent stories, they fall significantly short of human capabilities in terms of diversity and character detail. To address this, we introduce a novel story generation framework called CCI (Character-centric Creative story generation via Imagination). CCI features two innovative modules for creative story generation: IG (Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we utilize DALL-E 3 to create visual representations of key story elements. The IG generates more novel and concrete characters, backgrounds, and main plots than text-only methods. The MW module uses these story elements created by IG to generate multiple description candidates for the protagonist and select the best one. This method incorporates vivid and rich character descriptions into the story. We compared the stories generated by CCI and baseline models through human evaluation and statistical analysis. The results showed significant improvements in the creativity. Furthermore, by enabling interactive multi-modal story generation with users, we have opened up possibilities for human-LLM integration in cultural development.},
      journal={arXiv prerprint},
      pdf={https://arxiv.org/abs/2409.16667},
      website={https://kyeongman-header.github.io/CCI2024/},
      preview={CCI.png}
}


@article{kim2023lifetox,
  title={LifeTox: Unveiling Implicit Toxicity in Life Advice},
  author={Kim, Minbeom and Koo, Jahyun and Lee, Hwanhee and Park‚Ä†, Joonsuk and Lee, Hwaran and Jung‚Ä†, Kyomin},
  journal={NAACL 2024 (Oral, Top 5.9%)},
  pdf={https://aclanthology.org/2024.naacl-short.60/},
  abstract={As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity.},
  month={Apr},  
  year={2024},
  dataset={https://huggingface.co/datasets/mbkim/LifeTox},
  slides={https://minbeomkim.github.io/assets/pdf/LifeTox_Slides.pdf},
  model={https://huggingface.co/mbkim/LifeTox_Moderator_13B},
  preview={LifeTox.png}
}

@article{kim-etal-2023-critic,
    title = "Critic-Guided Decoding for Controlled Text Generation",
    author = "Kim, Minbeom  and
      Lee, Hwanhee  and
      Yoo, Kang Min  and
      Park, Joonsuk  and
      Lee‚Ä†, Hwaran  and
      Jung‚Ä†, Kyomin",
    journal = "ACL 2023 (Findings Spotlights)",
    month= "Jul",
    year = "2023",
    pdf = "https://aclanthology.org/2023.findings-acl.281",
    code = "https://github.com/minbeomkim/CriticControl",
    slides = "https://minbeomkim.github.io/assets/pdf/ACL2023-CriticControl-Slides.pdf",
    abstract = "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
    preview = "CriticControl.png"
}

@article{kim2022action,
  title={Action-driven contrastive representation for reinforcement learning},
  author={Kim, Minbeom and Rho, Kyeongha and Kim, Yong-duk and Jung‚Ä†, Kyomin},
  journal={Plos one (IF=3.7)},
  volume={17},
  number={3},
  pages={e0265456},
  year={2022},
  pdf={https://minbeomkim.github.io/assets/pdf/ADAT.pdf},
  abstract={In reinforcement learning, reward-driven feature learning directly from high-dimensional images faces two challenges: sample-efficiency for solving control tasks and generalization to unseen observations. In prior works, these issues have been addressed through learning representation from pixel inputs. However, their representation faced the limitations of being vulnerable to the high diversity inherent in environments or not taking the characteristics for solving control tasks. To attenuate these phenomena, we propose the novel contrastive representation method, Action-Driven Auxiliary Task (ADAT), which forces a representation to concentrate on essential features for deciding actions and ignore control-irrelevant details. In the augmented state-action dictionary of ADAT, the agent learns representation to maximize agreement between observations sharing the same actions. The proposed method significantly outperforms model-free and model-based algorithms in the Atari and OpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization.},
  publisher={Public Library of Science San Francisco, CA USA},
  preview={ADAT.png}
}

